{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6781a0c-3ab4-40e9-935b-3578e15233a1",
   "metadata": {},
   "source": [
    "# Utility Functions for Training and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15054e27-6d3a-4087-91ac-133b0f8095d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.utils import first\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from monai.losses import DiceLoss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22ee16d-fd91-4f1f-a7f6-77433bced882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_metric(predicted, target):\n",
    "    '''\n",
    "    In this function we take `predicted` and `target` (label) to calculate the dice coeficient then we use it \n",
    "    to calculate a metric value for the training and the validation.\n",
    "    '''\n",
    "    dice_value = DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)\n",
    "    value = 1 - dice_value(predicted, target).item()\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8239a20e-ac70-4fa9-a904-edfbd3453e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(val1, val2):\n",
    "    '''\n",
    "    In this function we take the number of the background and the forgroud pixels to return the `weights` \n",
    "    for the cross entropy loss values.\n",
    "    '''\n",
    "    count = np.array([val1, val2])\n",
    "    summ = count.sum()\n",
    "    weights = count/summ\n",
    "    weights = 1/weights\n",
    "    summ = weights.sum()\n",
    "    weights = weights/summ\n",
    "    return torch.tensor(weights, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51489419-5880-46ff-8aab-3244b745e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_in, loss, optim, max_epochs, model_dir, test_interval=1 , device=torch.device(\"cuda:0\")):\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    save_loss_train = []\n",
    "    save_loss_test = []\n",
    "    save_metric_train = []\n",
    "    save_metric_test = []\n",
    "    train_loader, test_loader = data_in\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "        model.train()\n",
    "        train_epoch_loss = 0\n",
    "        train_step = 0\n",
    "        epoch_metric_train = 0\n",
    "        for batch_data in train_loader:\n",
    "            \n",
    "            train_step += 1\n",
    "\n",
    "            volume = batch_data[\"vol\"]\n",
    "            label = batch_data[\"seg\"]\n",
    "            label = label != 0\n",
    "            volume, label = (volume.to(device), label.to(device))\n",
    "\n",
    "            optim.zero_grad()\n",
    "            outputs = model(volume)\n",
    "            \n",
    "            train_loss = loss(outputs, label)\n",
    "            \n",
    "            train_loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            train_epoch_loss += train_loss.item()\n",
    "            print(\n",
    "                f\"{train_step}/{len(train_loader) // train_loader.batch_size}, \"\n",
    "                f\"Train_loss: {train_loss.item():.4f}\")\n",
    "\n",
    "            train_metric = dice_metric(outputs, label)\n",
    "            epoch_metric_train += train_metric\n",
    "            print(f'Train_dice: {train_metric:.4f}')\n",
    "\n",
    "        print('-'*20)\n",
    "        \n",
    "        train_epoch_loss /= train_step\n",
    "        print(f'Epoch_loss: {train_epoch_loss:.4f}')\n",
    "        save_loss_train.append(train_epoch_loss)\n",
    "        np.save(os.path.join(model_dir, 'loss_train.npy'), save_loss_train)\n",
    "        \n",
    "        epoch_metric_train /= train_step\n",
    "        print(f'Epoch_metric: {epoch_metric_train:.4f}')\n",
    "\n",
    "        save_metric_train.append(epoch_metric_train)\n",
    "        np.save(os.path.join(model_dir, 'metric_train.npy'), save_metric_train)\n",
    "\n",
    "        if (epoch + 1) % test_interval == 0:\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_epoch_loss = 0\n",
    "                test_metric = 0\n",
    "                epoch_metric_test = 0\n",
    "                test_step = 0\n",
    "\n",
    "                for test_data in test_loader:\n",
    "\n",
    "                    test_step += 1\n",
    "\n",
    "                    test_volume = test_data[\"vol\"]\n",
    "                    test_label = test_data[\"seg\"]\n",
    "                    test_label = test_label != 0\n",
    "                    test_volume, test_label = (test_volume.to(device), test_label.to(device),)\n",
    "                    \n",
    "                    test_outputs = model(test_volume)\n",
    "                    \n",
    "                    test_loss = loss(test_outputs, test_label)\n",
    "                    test_epoch_loss += test_loss.item()\n",
    "                    test_metric = dice_metric(test_outputs, test_label)\n",
    "                    epoch_metric_test += test_metric\n",
    "                    \n",
    "                \n",
    "                test_epoch_loss /= test_step\n",
    "                print(f'test_loss_epoch: {test_epoch_loss:.4f}')\n",
    "                save_loss_test.append(test_epoch_loss)\n",
    "                np.save(os.path.join(model_dir, 'loss_test.npy'), save_loss_test)\n",
    "\n",
    "                epoch_metric_test /= test_step\n",
    "                print(f'test_dice_epoch: {epoch_metric_test:.4f}')\n",
    "                save_metric_test.append(epoch_metric_test)\n",
    "                np.save(os.path.join(model_dir, 'metric_test.npy'), save_metric_test)\n",
    "\n",
    "                if epoch_metric_test > best_metric:\n",
    "                    best_metric = epoch_metric_test\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    torch.save(model.state_dict(), os.path.join(\n",
    "                        model_dir, \"best_metric_model.pth\"))\n",
    "                \n",
    "                print(\n",
    "                    f\"current epoch: {epoch + 1} current mean dice: {test_metric:.4f}\"\n",
    "                    f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "                    f\"at epoch: {best_metric_epoch}\"\n",
    "                )\n",
    "\n",
    "\n",
    "    print(\n",
    "        f\"train completed, best_metric: {best_metric:.4f} \"\n",
    "        f\"at epoch: {best_metric_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfdde04e-cf9b-4928-9e38-a3a6570316d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_patient(data, SLICE_NUMBER=1, train=True, test=False):\n",
    "    \"\"\"\n",
    "    This function is to show one patient from your datasets, so that you can si if the it is okay or you need \n",
    "    to change/delete something.\n",
    "\n",
    "    `data`: this parameter should take the patients from the data loader, which means you need to can the function\n",
    "    prepare first and apply the transforms that you want after that pass it to this function so that you visualize \n",
    "    the patient with the transforms that you want.\n",
    "    `SLICE_NUMBER`: this parameter will take the slice number that you want to display/show\n",
    "    `train`: this parameter is to say that you want to display a patient from the training data (by default it is true)\n",
    "    `test`: this parameter is to say that you want to display a patient from the testing patients.\n",
    "    \"\"\"\n",
    "\n",
    "    check_patient_train, check_patient_test = data\n",
    "\n",
    "    view_train_patient = first(check_patient_train)\n",
    "    view_test_patient = first(check_patient_test)\n",
    "\n",
    "    \n",
    "    if train:\n",
    "        plt.figure(\"Visualization Train\", (12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(f\"vol {SLICE_NUMBER}\")\n",
    "        plt.imshow(view_train_patient[\"vol\"][0, 0, :, :, SLICE_NUMBER], cmap=\"gray\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(f\"seg {SLICE_NUMBER}\")\n",
    "        plt.imshow(view_train_patient[\"seg\"][0, 0, :, :, SLICE_NUMBER])\n",
    "        plt.show()\n",
    "    \n",
    "    if test:\n",
    "        plt.figure(\"Visualization Test\", (12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(f\"vol {SLICE_NUMBER}\")\n",
    "        plt.imshow(view_test_patient[\"vol\"][0, 0, :, :, SLICE_NUMBER], cmap=\"gray\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(f\"seg {SLICE_NUMBER}\")\n",
    "        plt.imshow(view_test_patient[\"seg\"][0, 0, :, :, SLICE_NUMBER])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45883ac9-7ca5-4692-b1d2-86c81178507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pixels(data):\n",
    "    val = np.zeros((1, 2))\n",
    "\n",
    "    for batch in tqdm(data):\n",
    "        batch_label = batch[\"seg\"] != 0\n",
    "        _, count = np.unique(batch_label, return_counts=True)\n",
    "\n",
    "        if len(count) == 1:\n",
    "            count = np.append(count, 0)\n",
    "        val += count\n",
    "\n",
    "    print('The last values:', val)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26713e-dbcf-4ecd-82a9-9f62be2c4374",
   "metadata": {},
   "source": [
    "# Pre-processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1351ef87-f00d-4d1c-9426-f69b2c4aad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.losses import DiceLoss, DiceCELoss\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63b79bf4-3968-4fb1-9fa8-e4d21a676865",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:/Youtube/Organ and Tumor Segmentation/datasets/Task03_Liver/Data_Train_Test'\n",
    "model_dir = 'D:/Youtube/Organ and Tumor Segmentation/results/results' \n",
    "# data_in = prepare(data_dir, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dd2cf21-dc38-4df2-bbbb-37cb08923b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = DiceCELoss(to_onehot_y=True, sigmoid=True, squared_pred=True, ce_weight=calculate_weights(1792651250,2510860).to(device))\n",
    "loss_function = DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-5, weight_decay=1e-5, amsgrad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef1113-d872-4548-9364-88e54d26f63d",
   "metadata": {},
   "source": [
    "# PostProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "865163b2-aca4-4100-915a-4959b35c7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import dicom2nifti\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from monai.transforms import (\n",
    "    LoadImage,\n",
    "    Resized,\n",
    "    ToTensor,\n",
    "    Spacing,\n",
    "    Orientation,\n",
    "    ScaleIntensityRange,\n",
    "    CropForeground,\n",
    ")\n",
    "\n",
    "from monai.data import DataLoader, Dataset, CacheDataset\n",
    "from monai.utils import set_determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9225f6d-7198-4992-a1d0-75a792a85dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groups(in_dir, out_dir, Number_slices):\n",
    "    '''\n",
    "    This function is to get the last part of the path so that we can use it to name the folder.\n",
    "    `in_dir`: the path to your folders that contain dicom files\n",
    "    `out_dir`: the path where you want to put the converted nifti files\n",
    "    `Number_slices`: here you put the number of slices that you need for your project and it will \n",
    "    create groups with this number.\n",
    "    '''\n",
    "\n",
    "    for patient in glob(in_dir + '/*'):\n",
    "        patient_name = os.path.basename(os.path.normpath(patient))\n",
    "\n",
    "        # Here we need to calculate the number of folders which mean into how many groups we will divide the number of slices\n",
    "        number_folders = int(len(glob(patient + '/*')) / Number_slices)\n",
    "\n",
    "        for i in range(number_folders):\n",
    "            output_path = os.path.join(out_dir, patient_name + '_' + str(i))\n",
    "            os.mkdir(output_path)\n",
    "\n",
    "            # Move the slices into a specific folder so that you will save memory in your desk\n",
    "            for i, file in enumerate(glob(patient + '/*')):\n",
    "                if i == Number_slices + 1:\n",
    "                    break\n",
    "                \n",
    "                shutil.move(file, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "692816bc-9e0c-4eab-b405-ac01700b98d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcm2nifti(in_dir, out_dir):\n",
    "    '''\n",
    "    This function will be used to convert dicoms into nifti files after creating the groups with \n",
    "    the number of slices that you want.\n",
    "    `in_dir`: the path to the folder where you have all the patients (folder of all the groups).\n",
    "    `out_dir`: the path to the output, which means where you want to save the converted nifties.\n",
    "    '''\n",
    "\n",
    "    for folder in tqdm(glob(in_dir + '/*')):\n",
    "        patient_name = os.path.basename(os.path.normpath(folder))\n",
    "        dicom2nifti.dicom_series_to_nifti(folder, os.path.join(out_dir, patient_name + '.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78f97123-1b9e-4930-8f7e-dd794bffa2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_empy(in_dir):\n",
    "    '''\n",
    "    This function will help you to find the empty volumes that you may not need for your training\n",
    "    so instead of opening all the files and search for the empty ones, them use this function to make it quick.\n",
    "    '''\n",
    "    \n",
    "    list_patients = []\n",
    "    for patient in glob(os.path.join(in_dir, '*')):\n",
    "        img = nib.load(patient)\n",
    "\n",
    "        if len(np.unique(img.get_fdata())) > 2:\n",
    "            print(os.path.basename(os.path.normpath(patient)))\n",
    "            list_patients.append(os.path.basename(os.path.normpath(patient)))\n",
    "    \n",
    "    return list_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d558882-4c9e-4301-9a01-92346b594810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(in_dir, pixdim=(1.5, 1.5, 1.0), a_min=-200, a_max=200, spatial_size=[128,128,64], cache=False):\n",
    "\n",
    "    \"\"\"\n",
    "    This function is for preprocessing, it contains only the basic transforms, but you can add more operations that you \n",
    "    find in the Monai documentation.\n",
    "    https://monai.io/docs.html\n",
    "    \"\"\"\n",
    "\n",
    "    set_determinism(seed=0)\n",
    "\n",
    "    path_train_volumes = sorted(glob(os.path.join(in_dir, \"TrainVolumes\", \"*.nii.gz\")))\n",
    "    path_train_segmentation = sorted(glob(os.path.join(in_dir, \"TrainSegmentation\", \"*.nii.gz\")))\n",
    "\n",
    "    path_test_volumes = sorted(glob(os.path.join(in_dir, \"TestVolumes\", \"*.nii.gz\")))\n",
    "    path_test_segmentation = sorted(glob(os.path.join(in_dir, \"TestSegmentation\", \"*.nii.gz\")))\n",
    "\n",
    "    train_files = [{\"vol\": image_name, \"seg\": label_name} for image_name, label_name in zip(path_train_volumes, path_train_segmentation)]\n",
    "    test_files = [{\"vol\": image_name, \"seg\": label_name} for image_name, label_name in zip(path_test_volumes, path_test_segmentation)]\n",
    "\n",
    "    train_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"vol\", \"seg\"]),\n",
    "            AddChanneld(keys=[\"vol\", \"seg\"]),\n",
    "            Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n",
    "            Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n",
    "            ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True), \n",
    "            CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n",
    "            Resized(keys=[\"vol\", \"seg\"], spatial_size=spatial_size),   \n",
    "            ToTensord(keys=[\"vol\", \"seg\"]),\n",
    "\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"vol\", \"seg\"]),\n",
    "            AddChanneld(keys=[\"vol\", \"seg\"]),\n",
    "            Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n",
    "            Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n",
    "            ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max,b_min=0.0, b_max=1.0, clip=True), \n",
    "            CropForegroundd(keys=['vol', 'seg'], source_key='vol'),\n",
    "            Resized(keys=[\"vol\", \"seg\"], spatial_size=spatial_size),   \n",
    "            ToTensord(keys=[\"vol\", \"seg\"]),\n",
    "\n",
    "            \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if cache:\n",
    "        train_ds = CacheDataset(data=train_files, transform=train_transforms,cache_rate=1.0)\n",
    "        train_loader = DataLoader(train_ds, batch_size=1)\n",
    "\n",
    "        test_ds = CacheDataset(data=test_files, transform=test_transforms, cache_rate=1.0)\n",
    "        test_loader = DataLoader(test_ds, batch_size=1)\n",
    "\n",
    "        return train_loader, test_loader\n",
    "\n",
    "    else:\n",
    "        train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "        train_loader = DataLoader(train_ds, batch_size=1)\n",
    "\n",
    "        test_ds = Dataset(data=test_files, transform=test_transforms)\n",
    "        test_loader = DataLoader(test_ds, batch_size=1)\n",
    "\n",
    "        return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c89645-bc5d-456b-a5b0-9de53f4c653a",
   "metadata": {},
   "source": [
    "# Creation of the UNET Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baab6427-8958-4435-9570-2fad9172aa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=(16, 32, 64, 128, 256), \n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    norm=Norm.BATCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6aaa4ebd-abc1-438d-a70d-4298034b8456",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_in' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     train(model, \u001b[43mdata_in\u001b[49m, loss_function, optimizer, \u001b[38;5;241m600\u001b[39m, model_dir)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_in' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train(model, data_in, loss_function, optimizer, 600, model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80faa824-87db-41f0-85bc-49b49a9feab9",
   "metadata": {},
   "source": [
    "# Testing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44989246-2b4d-4ef0-946e-fe3f07d82875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import(\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    Resized,\n",
    "    ToTensord,\n",
    "    Spacingd,\n",
    "    Orientationd,\n",
    "    ScaleIntensityRanged,\n",
    "    CropForegroundd,\n",
    "    Activations,\n",
    ")\n",
    "\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.data import CacheDataset, DataLoader, Dataset\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "from monai.inferers import sliding_window_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64d8ff71-7544-4e99-915e-77d99e89e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = 'D:/Youtube/Organ and Tumor Segmentation/datasets/Task03_Liver/Data_Train_Test'\n",
    "model_dir = 'D:/Youtube/Organ and Tumor Segmentation/results/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "098cb857-73b3-433e-8dd3-c7c3426e6795",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/Youtube/Organ and Tumor Segmentation/results/results\\\\loss_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss_train.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m train_metric \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric_train.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\med_seg_kidney\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/Youtube/Organ and Tumor Segmentation/results/results\\\\loss_train.npy'"
     ]
    }
   ],
   "source": [
    "train_loss = np.load(os.path.join(model_dir, 'loss_train.npy'))\n",
    "train_metric = np.load(os.path.join(model_dir, 'metric_train.npy'))\n",
    "test_loss = np.load(os.path.join(model_dir, 'loss_test.npy'))\n",
    "test_metric = np.load(os.path.join(model_dir, 'metric_test.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "125d52c1-7aae-43fd-adc1-f86b9d41bf6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain dice loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m x \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_loss\u001b[49m))]\n\u001b[0;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m train_loss\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loss' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(\"Results 25 june\", (12, 6))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title(\"Train dice loss\")\n",
    "x = [i + 1 for i in range(len(train_loss))]\n",
    "y = train_loss\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title(\"Train metric DICE\")\n",
    "x = [i + 1 for i in range(len(train_metric))]\n",
    "y = train_metric\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title(\"Test dice loss\")\n",
    "x = [i + 1 for i in range(len(test_loss))]\n",
    "y = test_loss\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title(\"Test metric DICE\")\n",
    "x = [i + 1 for i in range(len(test_metric))]\n",
    "y = test_metric\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "189ad054-f2d6-4b04-9c30-2df7aaeeee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_volumes = sorted(glob(os.path.join(in_dir, \"TrainVolumes\", \"*.nii.gz\")))\n",
    "path_train_segmentation = sorted(glob(os.path.join(in_dir, \"TrainSegmentation\", \"*.nii.gz\")))\n",
    "\n",
    "path_test_volumes = sorted(glob(os.path.join(in_dir, \"TestVolumes\", \"*.nii.gz\")))\n",
    "path_test_segmentation = sorted(glob(os.path.join(in_dir, \"TestSegmentation\", \"*.nii.gz\")))\n",
    "\n",
    "train_files = [{\"vol\": image_name, \"seg\": label_name} for image_name, label_name in zip(path_train_volumes, path_train_segmentation)]\n",
    "test_files = [{\"vol\": image_name, \"seg\": label_name} for image_name, label_name in zip(path_test_volumes, path_test_segmentation)]\n",
    "test_files = test_files[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d9b64-032b-4971-8bb3-952d9e004772",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"vol\", \"seg\"]),\n",
    "        AddChanneld(keys=[\"vol\", \"seg\"]),\n",
    "        Spacingd(keys=[\"vol\", \"seg\"], pixdim=(1.5,1.5,1.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n",
    "        ScaleIntensityRanged(keys=[\"vol\"], a_min=-200, a_max=200,b_min=0.0, b_max=1.0, clip=True), \n",
    "        CropForegroundd(keys=['vol', 'seg'], source_key='vol'),\n",
    "        Resized(keys=[\"vol\", \"seg\"], spatial_size=[128,128,64]),   \n",
    "        ToTensord(keys=[\"vol\", \"seg\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc0fbd8-e0ee-41c1-9960-7cdf6c28231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = Dataset(data=test_files, transform=test_transforms)\n",
    "test_loader = DataLoader(test_ds, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf2cc5-fb0a-4e87-9e31-50f026147fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model = UNet(\n",
    "    dimensions=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=(16, 32, 64, 128, 256), \n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    norm=Norm.BATCH,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879acd32-11c8-4a84-9e62-8510533f65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\n",
    "    os.path.join(model_dir, \"best_metric_model.pth\")))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c612add6-ec24-4e6b-b61c-e8bfba82f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_batch_size = 4\n",
    "roi_size = (128, 128, 64)\n",
    "with torch.no_grad():\n",
    "    test_patient = first(test_loader)\n",
    "    t_volume = test_patient['vol']\n",
    "    #t_segmentation = test_patient['seg']\n",
    "    \n",
    "    test_outputs = sliding_window_inference(t_volume.to(device), roi_size, sw_batch_size, model)\n",
    "    sigmoid_activation = Activations(sigmoid=True)\n",
    "    test_outputs = sigmoid_activation(test_outputs)\n",
    "    test_outputs = test_outputs > 0.53\n",
    "        \n",
    "    for i in range(32):\n",
    "        # plot the slice [:, :, 80]\n",
    "        plt.figure(\"check\", (18, 6))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(f\"image {i}\")\n",
    "        plt.imshow(test_patient[\"vol\"][0, 0, :, :, i], cmap=\"gray\")\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(f\"label {i}\")\n",
    "        plt.imshow(test_patient[\"seg\"][0, 0, :, :, i] != 0)\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(f\"output {i}\")\n",
    "        plt.imshow(test_outputs.detach().cpu()[0, 1, :, :, i])\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
